{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7Xmo6WPzlZz",
    "outputId": "9f84ecd7-bd21-41c3-be1e-5ecfa1051680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U trl accelerate git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q datasets bitsandbytes wandb\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v80HwviN0IRL",
    "outputId": "e29a6f90-84b3-424f-e839-124003f5bf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Am9OpI1L4Vl1",
    "outputId": "f016ad73-efed-46e8-f6ed-d6b6cf38bbee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1701795125885,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "Am9OpI1L4Vl1",
    "outputId": "7360c807-0b76-4004-b280-724a1c002a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer, AutoModelForQuestionAnswering\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model,AutoPeftModelForCausalLM\n",
    "import os,torch, wandb, platform, warnings\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncChromiumLoader, HuggingFaceDatasetLoader, DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain.chains import RunnablePassthrough, llm_chain\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import nest_asyncio\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vIGZL2_exPHd"
   },
   "outputs": [],
   "source": [
    "with open('train_webmd_squad_v2_consec.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_context_answer_triples(data):\n",
    "    question_context_answer_triples = []\n",
    "\n",
    "    for item in data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answers = [answer_info['text'] for answer_info in qa['answers']]\n",
    "                for answer in answers:\n",
    "                    question_context_answer_triples.append((question, context, answer))\n",
    "\n",
    "    return question_context_answer_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_context_answer_triples = extract_question_context_answer_triples(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(question_context_answer_triples, columns=['Question', 'Context', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Context</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What surgical techniques are used to treat gla...</td>\n",
       "      <td>Treatment of open-angle glaucoma -- the most c...</td>\n",
       "      <td>If the glaucoma does not respond to medication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the best ways to treat glaucoma?</td>\n",
       "      <td>Treatment of open-angle glaucoma -- the most c...</td>\n",
       "      <td>Both drugs and surgery have high rates of succ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should you know about treating open-angle...</td>\n",
       "      <td>Treatment of open-angle glaucoma -- the most c...</td>\n",
       "      <td>That is why it's so important to have your eye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is surgery for glaucoma dangerous?</td>\n",
       "      <td>Treatment of open-angle glaucoma -- the most c...</td>\n",
       "      <td>Before giving your consent, always ask the sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How is acute closed-angle glaucoma treated?</td>\n",
       "      <td>Treatment of open-angle glaucoma -- the most c...</td>\n",
       "      <td>Acute angle-closure glaucoma is different from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What surgical techniques are used to treat gla...   \n",
       "1          What are the best ways to treat glaucoma?   \n",
       "2  What should you know about treating open-angle...   \n",
       "3                 Is surgery for glaucoma dangerous?   \n",
       "4        How is acute closed-angle glaucoma treated?   \n",
       "\n",
       "                                             Context  \\\n",
       "0  Treatment of open-angle glaucoma -- the most c...   \n",
       "1  Treatment of open-angle glaucoma -- the most c...   \n",
       "2  Treatment of open-angle glaucoma -- the most c...   \n",
       "3  Treatment of open-angle glaucoma -- the most c...   \n",
       "4  Treatment of open-angle glaucoma -- the most c...   \n",
       "\n",
       "                                              Answer  \n",
       "0  If the glaucoma does not respond to medication...  \n",
       "1  Both drugs and surgery have high rates of succ...  \n",
       "2  That is why it's so important to have your eye...  \n",
       "3  Before giving your consent, always ask the sur...  \n",
       "4  Acute angle-closure glaucoma is different from...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19989, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Onjnev4z10YA"
   },
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "91fc5b8b2ab44172acb853d283a2e6a9",
      "799feff152df49e9a1ebc26ed42f2995",
      "9fbd6643b4a442bfa44377e19d34d453",
      "8959c048bfb344818fbd3099b02dd31f",
      "4dcdd3115d9d4a1bb305466a57b10cc8",
      "18b91caca9c9438bb97fbd84f81e07b2",
      "34e3250a8f1840e8abc6f31a0def4ee1",
      "b5038960f8dd4de2bb54fc6613d6bdd1",
      "a6e011cb6e4e47cebb17b611437ef8b9",
      "c7b2e06c1561455998546a5ddd05997a",
      "ea92b0d6ea20460c946169045c2a4faa",
      "19dcb76000734120ba1838753e1e29e3",
      "2c9f309ef4474982b496aed308e64541",
      "8d9d4d292d69409689f903d99a59fda0",
      "4abac84ee3e84415ac5d502964f97e72",
      "68876547146d47a5b0bbce69e2a763d6",
      "03763433ff9a4fd2874abb9ad034260a",
      "9409919e903a4ebe85c6338ba1bff4aa",
      "4c0cfb4be4f743a9af8c8468374eec92",
      "cc69790114db41bcafcabd8cff380b50",
      "b03fe16c4e624e88a85e2a6a68bbb8b5",
      "1f9b9ac166294206ace77b3a50054a3d",
      "4d8f32abfda54301809247936b9b54d7",
      "7a7187fc9452486aad2d907dd414ffd5",
      "bcb97b7c3e374b6d97c71d9b7f7e4062",
      "9d89be2492294db19d267fc6acd18604",
      "2101ed0500c342b799177e1ccaacf1f8",
      "61a726eb7fc547ee9a465398c9f45e2e",
      "a3634a74e648497584ea686fcf28a2c2",
      "27af2ea65da64811a3b85974136cff4e",
      "c4a6b813b3914a2fb13650597bb3c21f",
      "93c89ab1bb244a2aa8cf51e4c1a1039b",
      "5a93db9179f4491f92126e813e0d462f",
      "ae54ca82c7fc425dbe3c3a3227598bd9",
      "6239e1e2a65d4733ae780d6ecaa3a4ee",
      "16ca639215604d50922eb43cf514c30b",
      "74217aaeb5114a158cdf0ca8c81b919f",
      "a36af20c961e4c59a0c7af92ce331c4e",
      "c89e9320a01d460eb3135c03e9f5dc19",
      "43252bf33e44478db788d9897c629b05",
      "a2d69f7aad6c457f8ca79a87152e8b70",
      "17a7bb570b914517aa474209136afa67",
      "fe849d1fa1124dd88526a278765a7e6a",
      "3a07408681de4644bd35a366866bf69f",
      "aa6cb6d1f234466189afbd7f87f9fa89",
      "4b85cd5c84694ac88d2d8ec8e97849f4",
      "ce2599a839074d1393df2eb43a8be949",
      "da0353eab48340a6a42b5336e0f199ed",
      "f12b3f50f91b4f5d9b8c3eb6b356c76d",
      "9196e29878524e1f856fea3508b99bf9",
      "996185e3aa09432bacab12407c53596d",
      "d62f9cf79a5e43fbbf854620bdbba355",
      "16fd3fa967434963b784627e1ad743ae",
      "0a93054e51a3460fb26e46d50df5de36",
      "5b6ba938780549879712eae15a236caf"
     ]
    },
    "executionInfo": {
     "elapsed": 19367,
     "status": "ok",
     "timestamp": 1701796575198,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "jNoSCHqX1psJ",
    "outputId": "e007c88d-54cc-4e5f-e191-620814a4454d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0967613412d4ac0a69abb7806a3ce47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=bnb_config,\n",
    "    # use_cache=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMjxo66D_LTH"
   },
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XDlGdp5-zy6"
   },
   "source": [
    "#### **1. Zero-shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1701797012883,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "YCQ4NyTI-xCK"
   },
   "outputs": [],
   "source": [
    "prompt1 = \"What types of exercise are best for people with asthma?\"\n",
    "prompt2 = \"How is obsessive-compulsive disorder diagnosed?\"\n",
    "prompt3 = \"When are you more likely to get a blood clot?\"\n",
    "prompt4 = \"How should you lift objects to prevent back pain?\"\n",
    "prompt5 = \"How can you be smart with antibiotics?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23994,
     "status": "ok",
     "timestamp": 1701797373160,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "pZ0KpOjj_aK0",
    "outputId": "7cb018dd-1fea-4591-c727-eb3f252ee620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What types of exercise are best for people with asthma?\n",
      "\n",
      "Asthma is a chronic lung disease that causes inflammation and narrowing of the airways. It can cause shortness of breath, wheezing, coughing, and chest tightness.\n",
      "\n",
      "Exercise can be a trigger for asthma symptoms. But it can also help improve lung function and reduce the frequency of asthma attacks.\n",
      "\n",
      "The American College of Sports Medicine (ACSM) recommends that people with asthma exercise at least 30 minutes a day, five days a week.\n",
      "\n",
      "The ACSM also recommends that people with asthma exercise at a moderate intensity. This means that you should be able to talk, but not sing, while exercising.\n",
      "\n",
      "There are many different types of exercise that can be beneficial for people with asthma. Some of the most popular types of exercise include:\n",
      "\n",
      "- Walking\n",
      "- Swimming\n",
      "- Cycling\n",
      "- Yoga\n",
      "- Pilates\n",
      "- Tai chi\n",
      "\n",
      "Each of these types of exercise has its own benefits for people with asthma.\n",
      "\n",
      "Walking is a low-impact exercise\n"
     ]
    }
   ],
   "source": [
    "device = base_model.device\n",
    "\n",
    "inputs = tokenizer(prompt1, return_tensors=\"pt\").to(device)\n",
    "outputs = base_model.generate(**inputs, max_length=250, top_p=0.9, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24353,
     "status": "ok",
     "timestamp": 1701797437135,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "CSjkaWL_BQlr",
    "outputId": "78690654-867d-4aa7-aeac-24a9cd0bd0a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is obsessive-compulsive disorder diagnosed?\n",
      "\n",
      "Obsessive-compulsive disorder is diagnosed by a mental health professional, such as a psychiatrist or psychologist. The professional will conduct a clinical interview and may ask you to fill out a questionnaire.\n",
      "\n",
      "The Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), published by the American Psychiatric Association, is the standard classification of mental disorders used by mental health professionals in the United States. The DSM-5 criteria for obsessive-compulsive disorder include:\n",
      "\n",
      "- Obsessions or compulsions that are time-consuming (more than an hour a day) or cause significant distress or impairment in social, occupational, or other important areas of functioning.\n",
      "- The obsessions or compulsions are not due to the direct physiological effects of a substance (e.g., a drug of abuse, a medication) or a general medical condition (e.g., hyperthyroidism).\n",
      "\n",
      "The DSM-5 also includes criteria for obsessive-compulsive disorder subtypes, including:\n",
      "\n",
      "- O\n"
     ]
    }
   ],
   "source": [
    "device = base_model.device\n",
    "\n",
    "inputs = tokenizer(prompt2, return_tensors=\"pt\").to(device)\n",
    "outputs = base_model.generate(**inputs, max_length=250, top_p=0.9, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24184,
     "status": "ok",
     "timestamp": 1701797546690,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "Xe2QDSL2B9HJ",
    "outputId": "4752e390-524f-4ca6-e172-221e3807427a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When are you more likely to get a blood clot?\n",
      "\n",
      "- When you are pregnant\n",
      "- When you are taking birth control pills\n",
      "- When you are taking hormone replacement therapy\n",
      "- All of the above\n",
      "\n",
      "Answer: All of the above.\n",
      "\n",
      "Blood clots are a serious health concern for women. They can cause heart attacks, strokes, and pulmonary embolisms.\n",
      "\n",
      "The risk of blood clots is higher during pregnancy and when taking birth control pills or hormone replacement therapy.\n",
      "\n",
      "If you are pregnant, taking birth control pills, or taking hormone replacement therapy, you should be aware of the signs and symptoms of blood clots.\n",
      "\n",
      "If you experience any of the following symptoms, you should seek medical attention immediately:\n",
      "\n",
      "- Chest pain\n",
      "- Shortness of breath\n",
      "- Coughing up blood\n",
      "- Swelling in the legs\n",
      "- Pain in the legs\n",
      "- Redness or warmth in the legs\n",
      "\n",
      "If you are pregnant, taking birth control pills, or taking hormone replacement therapy, you should be aware of the signs and symptoms of blood clots.\n",
      "\n",
      "If you experience any of the following symptoms, you should seek medical\n"
     ]
    }
   ],
   "source": [
    "device = base_model.device\n",
    "\n",
    "inputs = tokenizer(prompt3, return_tensors=\"pt\").to(device)\n",
    "outputs = base_model.generate(**inputs, max_length=250, top_p=0.9, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24296,
     "status": "ok",
     "timestamp": 1701797645242,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "e9FBTSXWB_zz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f85069ce-7b79-43fe-884c-f54511739f0a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt4, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/generation/utils.py:1658\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1642\u001b[0m         input_ids,\n\u001b[1;32m   1643\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/generation/utils.py:2506\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2506\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2514\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:1049\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1046\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1062\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:936\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    929\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    930\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    931\u001b[0m         hidden_states,\n\u001b[1;32m    932\u001b[0m         attention_mask,\n\u001b[1;32m    933\u001b[0m         position_ids,\n\u001b[1;32m    934\u001b[0m     )\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:638\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    636\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 638\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:195\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:219\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    218\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 219\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:564\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul_4bit\u001b[39m(A: tensor, B: tensor, quant_state: List, out: tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m quant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:512\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_fp4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    515\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/functional.py:866\u001b[0m, in \u001b[0;36mdequantize_fp4\u001b[0;34m(A, quant_state, absmax, out, blocksize)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdequantize_fp4\u001b[39m(A: Tensor, quant_state: Tuple[Tensor, Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, absmax: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, out: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, blocksize: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mar39/lib/python3.9/site-packages/bitsandbytes/functional.py:933\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    931\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_fp4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(blocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m         \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdequantize_blockwise_fp16_nf4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlockwise quantization only supports 16/32-bit floats, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = base_model.device\n",
    "\n",
    "inputs = tokenizer(prompt4, return_tensors=\"pt\").to(device)\n",
    "outputs = base_model.generate(**inputs, max_length=250, top_p=0.9, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24454,
     "status": "ok",
     "timestamp": 1701797669673,
     "user": {
      "displayName": "Nida Fitriyah",
      "userId": "01905401224414903487"
     },
     "user_tz": 360
    },
    "id": "o6TuliKnCDyv",
    "outputId": "dd0c6caa-7742-46bf-e2d7-8086e4fed7ec"
   },
   "outputs": [],
   "source": [
    "device = base_model.device\n",
    "\n",
    "inputs = tokenizer(prompt5, return_tensors=\"pt\").to(device)\n",
    "outputs = base_model.generate(**inputs, max_length=250, top_p=0.9, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhWfje3vDt34"
   },
   "source": [
    "#### **2. Few-shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions and answers for few-shot learning\n",
    "few_shot_examples = [(\"What surgical techniques are used to treat glaucoma?\", \"If the glaucoma does not respond to medication, or if you cannot tolerate the side effects, your doctor may change medications or recommend one of several surgical techniques: Laser trabeculoplasty creates small laser burns in the area where the fluid drains, improving the outflow rate of aqueous fluid. This relatively brief procedure can often be done in an ophthalmologist's clinic. Trabeculectomy is a surgical procedure that creates a new channel for fluid outflow in cases in which the intraocular pressure is high and the optic nerve damage progresses. Long-term results vary, but generally, the success rate is good. Surgical implants that shunt fluid out of the eye may also be used to decrease pressure in the eye. Remember, all forms of medical or surgical treatment have potential benefits and risks. Before giving your consent, always ask the surgeon to clearly explain any treatment or surgery as well as the proposed benefits, effective alternatives, and potential risks or complications.\"),\n",
    "    (\"What are the best ways to treat glaucoma?\",\"Both drugs and surgery have high rates of success in treating chronic open-angle glaucoma, but you can help yourself by carefully following the doctor's treatment plan. Some patients may find it difficult to follow a regimen involving two or three different eye drops. Be candid and tell the doctor if you cannot follow the medication schedule or if the eye drops cause unwanted side effects. There are frequently alternative treatments. Because of potential drug interactions, be sure to tell your doctor about any other medical problems you have or other medications you take. If glaucoma drops causes the eyes to become chronically red, consult your doctor about switching to preservative-free glaucoma drops that may alleviate the redness from preservatives.\"),\n",
    "    (\"Is surgery for glaucoma dangerous?\",\"Before giving your consent, always ask the surgeon to clearly explain any treatment or surgery as well as the proposed benefits, effective alternatives, and potential risks or complications.\"),\n",
    "    (\"How is acute closed-angle glaucoma treated?\",\"Acute angle-closure glaucoma is different from chronic open-angle glaucoma in several important ways: The symptoms usually occur with relative suddenness; the eye is painful and red. If the high pressure in the eye is not relieved quickly, blindness can occur. On the other hand, treatments for acute angle-closure glaucoma -- usually laser treatment -- are permanent and do not require long-term therapy. For this type of glaucoma, making a hole in the iris to allow fluid to drain, called an iridectomy, is the standard treatment to cure it. The unaffected eye also is usually treated to prevent a future attack. However, it's important to get your eyes checked regularly, as some people may develop a case of chronic angle-closure glaucoma later in life, even after laser treatment.\"),\n",
    "    (\"What should you know about treating open-angle glaucoma?\",\"That is why it's so important to have your eye doctor test you regularly for glaucoma. Once diagnosed, glaucoma is usually controlled with eye drops that reduce eye pressure. Glaucoma is a life-long condition and needs continual follow-up with your eye doctor. Both drugs and surgery have high rates of success in treating chronic open-angle glaucoma, but you can help yourself by carefully following the doctor's treatment plan. Some patients may find it difficult to follow a regimen involving two or three different eye drops. Be candid and tell the doctor if you cannot follow the medication schedule or if the eye drops cause unwanted side effects. There are frequently alternative treatments. Because of potential drug interactions, be sure to tell your doctor about any other medical problems you have or other medications you take. If glaucoma drops causes the eyes to become chronically red, consult your doctor about switching to preservative-free glaucoma drops that may alleviate the redness from preservatives. Acute angle-closure glaucoma is different from chronic open-angle glaucoma in several important ways: The symptoms usually occur with relative suddenness; the eye is painful and red. If the high pressure in the eye is not relieved quickly, blindness can occur.\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "1. Swimming. Swimming is a great exercise for people with asthma. The humidity in the air and the warm temperature of the water can help to open up your airways. 2. Walking. Walking is a great exercise for people with asthma. It is low impact and can be done at your own pace. 3. Yoga. Yoga is a great exercise for people with asthma. The deep breathing exercises can help to open up your airways and the stretching can help to improve your flexibility. 4. Cycling. Cycling is a great exercise for people with asthma. It is low impact and can be done at your own pace. 5. Pilates. Pilates is a great exercise for people with asthma. The deep breathing exercises can help to open up your airways and the stretching can help to improve your flexibility. 6. Tai Chi. Tai Chi is a great exercise for people with asthma. The deep breathing exercises can help to open up your airways and the stretching can help to improve your flexibility. 7. Dancing. Dancing is a great exercise for people with asthma\n"
     ]
    }
   ],
   "source": [
    "# New question\n",
    "new_question = prompt1\n",
    "\n",
    "# Constructing the few-shot prompt with an explicit instruction\n",
    "prompt = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in few_shot_examples])\n",
    "# prompt += \"\\n\\nInstruction: Follow the pattern of the example questions and answers to answer the new question in 150  words.\"\n",
    "prompt += f\"\\n\\nQ: {new_question}\\nA: \"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "device = base_model.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Adjusting max_length if needed\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer_start = response.find(f\"Q: {new_question}\\nA: \") + len(f\"Q: {new_question}\\nA: \")\n",
    "answer = response[answer_start:].split(\"\\n\")[0].strip()\n",
    "\n",
    "# Print the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "1. The patient has obsessions, compulsions, or both. 2. The obsessions or compulsions are time-consuming (more than 1 hour a day) or cause marked distress or significant impairment in social, occupational, or other important areas of functioning. 3. The obsessions or compulsions are not due to the direct physiological effects of a substance (e.g., a drug of abuse, a medication) or a general medical condition (e.g., hyperthyroidism). 4. The obsessions or compulsions are not better accounted for by another mental disorder (e.g., anxiety disorder, body dysmorphic disorder, anorexia nervosa, obsessive-compulsive personality disorder).\n"
     ]
    }
   ],
   "source": [
    "new_question = prompt2\n",
    "\n",
    "# Constructing the few-shot prompt with an explicit instruction\n",
    "prompt = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in few_shot_examples])\n",
    "# prompt += \"\\n\\nInstruction: Follow the pattern of the example questions and answers to answer the new question in 150  words.\"\n",
    "prompt += f\"\\n\\nQ: {new_question}\\nA: \"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "device = base_model.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Adjusting max_length if needed\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer_start = response.find(f\"Q: {new_question}\\nA: \") + len(f\"Q: {new_question}\\nA: \")\n",
    "answer = response[answer_start:].split(\"\\n\")[0].strip()\n",
    "\n",
    "# Print the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "1. If you are over 60 years old. 2. If you are a woman. 3. If you have a family history of blood clots. 4. If you have had a blood clot before. 5. If you have had a heart attack or stroke. 6. If you have had surgery or an injury. 7. If you are pregnant. 8. If you are taking birth control pills. 9. If you are taking estrogen. 10. If you are taking certain medicines. 11. If you are overweight. 12. If you smoke. 13. If you are not active. 14. If you have a blood clotting disorder. 15. If you have a catheter in a vein. 16. If you have cancer. 17. If you have a central venous catheter. 18. If you have a pacemaker. 19. If you have a stent. 20. If you have a port-a-cath. 21. If you have a central venous catheter.\n"
     ]
    }
   ],
   "source": [
    "new_question = prompt3\n",
    "\n",
    "# Constructing the few-shot prompt with an explicit instruction\n",
    "prompt = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in few_shot_examples])\n",
    "# prompt += \"\\n\\nInstruction: Follow the pattern of the example questions and answers to answer the new question in 150  words.\"\n",
    "prompt += f\"\\n\\nQ: {new_question}\\nA: \"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "device = base_model.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Adjusting max_length if needed\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer_start = response.find(f\"Q: {new_question}\\nA: \") + len(f\"Q: {new_question}\\nA: \")\n",
    "answer = response[answer_start:].split(\"\\n\")[0].strip()\n",
    "\n",
    "# Print the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "1. Bend your knees and keep your back straight. 2. Lift with your legs, not your back. 3. Keep the object close to your body. 4. Avoid twisting your body. 5. Do not lift heavy objects. 6. Do not lift and hold objects for long periods. 7. Do not lift and carry objects above shoulder level. 8. Do not lift and carry objects below knee level. 9. Do not lift and carry objects that are too heavy for you. 10. Do not lift and carry objects that are too heavy for you. 11. Do not lift and carry objects that are too heavy for you. 12. Do not lift and carry objects that are too heavy for you. 13. Do not lift and carry objects that are too heavy for you. 14. Do not lift and carry objects that are too heavy for you. 15. Do not lift and carry objects that are too heavy for you. 16. Do not lift and carry objects that are too heavy for you. 17. Do not lift and carry objects that are too heavy for you. 18. Do\n"
     ]
    }
   ],
   "source": [
    "new_question = prompt4\n",
    "\n",
    "# Constructing the few-shot prompt with an explicit instruction\n",
    "prompt = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in few_shot_examples])\n",
    "# prompt += \"\\n\\nInstruction: Follow the pattern of the example questions and answers to answer the new question in 150  words.\"\n",
    "prompt += f\"\\n\\nQ: {new_question}\\nA: \"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "device = base_model.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Adjusting max_length if needed\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer_start = response.find(f\"Q: {new_question}\\nA: \") + len(f\"Q: {new_question}\\nA: \")\n",
    "answer = response[answer_start:].split(\"\\n\")[0].strip()\n",
    "\n",
    "# Print the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "1. Don't take antibiotics unless prescribed by a doctor. 2. Don't take antibiotics prescribed for someone else. 3. Don't take antibiotics for viral infections, such as colds, flu, and most sore throats. 4. Don't take antibiotics for a long time without a doctor's advice. 5. Don't take antibiotics for a long time without a doctor's advice. 6. Don't take antibiotics for a long time without a doctor's advice. 7. Don't take antibiotics for a long time without a doctor's advice. 8. Don't take antibiotics for a long time without a doctor's advice. 9. Don't take antibiotics for a long time without a doctor's advice. 10. Don't take antibiotics for a long time without a doctor's advice. 11. Don't take antibiotics for a long time without a doctor's advice. 12. Don't take antibiotics for a long time without a doctor'\n"
     ]
    }
   ],
   "source": [
    "new_question = prompt5\n",
    "\n",
    "# Constructing the few-shot prompt with an explicit instruction\n",
    "prompt = \"\\n\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in few_shot_examples])\n",
    "# prompt += \"\\n\\nInstruction: Follow the pattern of the example questions and answers to answer the new question in 150  words.\"\n",
    "prompt += f\"\\n\\nQ: {new_question}\\nA: \"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "device = base_model.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Adjusting max_length if needed\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer_start = response.find(f\"Q: {new_question}\\nA: \") + len(f\"Q: {new_question}\\nA: \")\n",
    "answer = response[answer_start:].split(\"\\n\")[0].strip()\n",
    "\n",
    "# Print the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Chain of Thought**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Model's Response:\n",
      " Context: Does it take you forever to make a doctor's appointment, clean out your garage, or do your taxes? Putting off something that needs to be done is called procrastination. We all do it sometimes. But if you constantly struggle to finish tasks, there may be a bigger problem at play. Once you figure out your reasons, you can work on making the most of your time. When you're facing something you don't want to do, it can make you feel tense. Putting off that dreaded chore is a way to get some short-term relief. The downside is that you'll still have to tackle your task in the future, which may make you feel guilty or angry -- and cause your stress to rise more. If the tension's bad enough to keep you from getting things done, you might notice it affects you in other ways, too, like: Trouble sleeping Racing thoughts Lack of energy or trouble focusing Headaches or muscle tension Keep your mood in check with regular exercise, limit alcohol and caffeine (which can make stress worse), and get enough sleep. Talk to a friend or counselor about what's on your mind, too. Attention-deficit hyperactivity disorder affects behavior. People who have it often have trouble making decisions or getting tasks done before a deadline. Some get too distracted with other activities around them. Others find it hard to plan ahead, or they get frustrated easily and give up. Other symptoms of ADHD include: Daydreaming Forgetting or losing things Making careless mistakes or taking unsafe risks Squirming or fidgeting Medications can often help control many symptoms of the disorder. Cognitive-behavioral therapy, which helps identify and change negative thought patterns, also can help many people with the condition better manage their time. The amount of willpower you have changes every day and depends on many things, including whether you've had enough sleep. If you've had less than 6 hours, it's harder for your brain to tune out distractions and focus long enough to finish what you need to do. Other signs that you need more shut-eye include: Falling asleep while watching TV or reading a book Feeling irritable Sleeping longer on weekends Trouble waking up in the mornings To get a good night's rest, keep the same bedtime and wake time. Don't smoke, and steer clear of alcohol, caffeine, and heavy meals for a few hours before bed, since they can disrupt your Zzz's. This condition means your brain overreacts to negative emotions. You expect the worst, even when there's nothing to fear. Some people with anxiety spend so much energy worrying about family, health, money, or work that they find it hard to carry out everyday tasks. You might also have: Muscle tension Fatigue Trouble sleeping Irritability To stop anxious feelings, take 10 slow, deep breaths, or replace a negative thought with a positive one. It's also important to get enough rest, exercise regularly, and make sure you don't skip meals. Keep a journal to help you learn what triggers your anxiety. Some people need medications or talk therapy to get the condition under control. Depression alters your brain's chemistry. You may have very little energy, even for hobbies and activities you love. It's also common to feel helpless and self-critical, which can cause someone with the condition to âshut down.â Other symptoms of depression include: A feeling of sadness that doesn't go away Low appetite and weight loss, or overeating and weight gain Restlessness or feeling irritable Thoughts of suicide or death Call 911 if you're thinking about hurting yourself or others. Even if not, talk to a counselor or therapist if you have any of these other symptoms. Your doctor is a good resource as well. Some people also find relief from their depression with antidepressant medicine. If you have this disorder, your brain can't signal when you're doing something correctly. Instead, you have a nagging sense that your actions aren't âjust right.â Rather than finish a task, people with OCD get stuck looking for a âperfectâ solution. Other symptoms include: A lot of unwanted thoughts or images Feeling helpless to stop the thoughts Rituals like hand washing Spending at least 1 hour a day on these thoughts and rituals Anti-anxiety medication or antidepressants can often improve OCD symptoms. Many people also use cognitive-behavioral therapy or exposure therapy, in which they learn to slowly face their fears. Be honest with yourself about what might be holding you back from getting a task done. Then think of realistic ways you can get past it. If you think you're stuck because of a more serious health problem, talk to your doctor to make sure you get the right diagnosis and treatment. If good old-fashioned procrastination is your issue, break big projects into smaller ones you can do more easily. You can set deadlines to stay on track. Some people also find it helpful to reward themselves when they make progress. When you catch yourself wasting time, forgive yourself rather than feeling ashamed or angry. You may be less likely to procrastinate again if you go easy on yourself.\n",
      "\n",
      "Question: How is obsessive-compulsive disorder diagnosed?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The diagnosis of OCD is based on the presence of obsessions, compulsions, or both.\n",
      "\n",
      "The obsessions are recurrent and persistent thoughts, urges, or images that are experienced as intrusive and inappropriate and cause anxiety or distress.\n",
      "\n",
      "The compulsions are repetitive behaviors or mental acts that the person feels driven to perform in response to an obsession or according to rules that must be applied rigidly.\n",
      "\n",
      "The obsessions or compulsions are time-consuming (more than 1 hour a day) or cause clinically significant distress or impairment in social, occupational, or other important areas of functioning.\n",
      "\n",
      "The obsessions or compulsions are not attributable to the physiological effects of a substance (e.g., a drug of abuse, a medication) or another medical condition (e.g., hyperthyroidism).\n",
      "\n",
      "The obsessions or compulsions are not better explained by the symptoms of another mental disorder (e.g., excessive worries, as in generalized anxiety disorder, or a preoccupation with as-sessed as in body dysmorphic disorder, or hair pulling [\n",
      "\n",
      "Model's Answer:\n",
      " The obsessions or compulsions are not better explained by the symptoms of another mental disorder (e.g., excessive worries, as in generalized anxiety disorder, or a preoccupation with as-sessed as in body dysmorphic disorder, or hair pulling [\n"
     ]
    }
   ],
   "source": [
    "# Function to find the question and context for questions containing 'asthma' but not 'BDD'\n",
    "def get_question_and_context_for_asthma_excluding_bdd(df):\n",
    "    # Filter the DataFrame for rows where the 'Question' contains 'asthma' and does not contain 'BDD'\n",
    "    asthma_rows = df[\n",
    "        df['Question'].str.contains(r'\\bOCD\\b', case=False, regex=True) &\n",
    "        ~df['Question'].str.contains(r'\\bBDD\\b', case=False, regex=True)\n",
    "    ]\n",
    "    \n",
    "    # If any rows are found, return the 'Question' and 'Context' of the first such row\n",
    "    if not asthma_rows.empty:\n",
    "        question = asthma_rows.iloc[0]['Question']\n",
    "        context = asthma_rows.iloc[0]['Context']\n",
    "        return question, context\n",
    "    else:\n",
    "        return \"No question with the keyword 'asthma' found, excluding 'BDD'.\", None\n",
    "\n",
    "# Call the function and print the results\n",
    "question_with_asthma, context_2 = get_question_and_context_for_asthma_excluding_bdd(df)\n",
    "\n",
    "# Example question based on the context\n",
    "question = \"How is obsessive-compulsive disorder diagnosed?\"\n",
    "\n",
    "# Constructing the prompt\n",
    "prompt = f\"Context: {context_2}\\n\\nQuestion: {question}\\n\\nInstruction: Based on the context, answer the question.\\n\"\n",
    "\n",
    "# Tokenize and generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full model's response for debugging\n",
    "print(\"\\nFull Model's Response:\\n\", response)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer = response.split('\\n')[-1].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Model's Response:\n",
      " Context: After hurting your leg, you're probably dealing with some discomfort and inconvenience. But that's not all you have to be concerned about. This type of injury raises your chances for getting a blood clot. Any time a blood vessel gets damaged, the nearby blood can thicken and organize into a sticky clump, or clot. Some clots only affect veins near your skin's surface. This condition, called superficial thrombophlebitis, typically doesn't lead to serious problems. When a blood clot forms farther inside your leg, it's known as deep vein thrombosis ( DVT). These clots can be dangerous if they break loose and travel to your lungs. Doctors call this a pulmonary embolism ( PE). Trauma could result from a car accident, a sports injury, or even a fall. Common mishaps that may lead to a clot include: Broken bones Bad bumps Severe bruises Severe muscle injuries A 2008 study revealed even minor leg injuries -- ones that don't need a cast or bed rest -- can raise your odds of having DVT. Researchers found as many as 1 in 13 blood clots may be caused by small problems, such as muscle tears or ankle sprains. Many people don't notice any symptoms. Spotting DVT could be tricky after an injury because a bruise or bump can look like a clot. DVT symptoms can be mistaken for a muscle tear, a charley horse, a twisted ankle, or shin splints. Your leg could: Swell Hurt or feel tender, maybe like a cramp Feel warm Look red or discolored Have veins that stick out If a clot moves to your lungs, you may: Cough up blood Have pain in your chest Feel your heart beat fast Have trouble breathing Hurt when you breathe Call your doctor if you notice anything unusual or worrisome after a leg injury. Some people are more likely than others to develop a blood clot. Your odds are higher when you: Have a close family member with DVT Are older Have a blood clotting disorder or a vein disease Have cancer Are pregnant Use birth control pills or hormone replacement therapy Being overweight and smoking will raise your chances. Also, being a couch potato can lead to a clot. While blood clots are less common in younger, healthy people, they're still possible. Fit athletes are likely to be injured, get dehydrated, and travel long distances for events. These things increase the odds of blood clots, too. You might still be hurting from your injury, but activity is key for keeping clots at bay. Don't sit or stand for more than an hour at a time if you can help it. Wear loose-fitting clothing. Your doctor might tell you to wear special compression stockings if you have a higher chance for developing a clot. Drink plenty of water, and stay away from alcohol, especially if you're traveling long distances. Try not to hurt your legs again during your recovery. Losing extra weight and quitting smoking can lower your chance of getting a blood clot.\n",
      "\n",
      "Question: When are you more likely to get a blood clot?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The context is about a blood clot.\n",
      "\n",
      "The answer is:\n",
      "\n",
      "When you have a close family member with DVT, are older, have a blood clotting disorder or a vein disease, have cancer, are pregnant, use birth control pills or hormone replacement therapy, are overweight, smoke, or are a couch potato.\n",
      "\n",
      "The answer is:\n",
      "\n",
      "When you have a close family member with DVT, are older, have a blood clotting disorder or a vein disease, have cancer, are pregnant, use birth control pills or hormone replacement therapy, are overweight, smoke, or are a couch potato.\n",
      "\n",
      "The answer is:\n",
      "\n",
      "When you have a close family member with DVT, are older, have a blood clotting disorder or a vein disease, have cancer, are pregnant, use birth control pills or hormone replacement therapy, are overweight, smoke, or are a couch potato.\n",
      "\n",
      "The answer is:\n",
      "\n",
      "When you have a close family member with DVT, are older, have a blood clotting disorder or a vein disease, have cancer, are pregnant, use birth\n",
      "\n",
      "Model's Answer:\n",
      " When you have a close family member with DVT, are older, have a blood clotting disorder or a vein disease, have cancer, are pregnant, use birth\n"
     ]
    }
   ],
   "source": [
    "# Function to find the question and context for questions containing 'asthma'\n",
    "def get_question_and_context_for_asthma(df):\n",
    "    # Filter the DataFrame for rows where the 'Question' column contains the word 'asthma'\n",
    "    asthma_rows = df[df['Question'].str.contains(r'\\bblood clot\\b', case=False, regex=True)]\n",
    "    \n",
    "    # If any rows are found, return the 'Question' and 'Context' of the first such row\n",
    "    if not asthma_rows.empty:\n",
    "        question = asthma_rows.iloc[0]['Question']\n",
    "        context = asthma_rows.iloc[0]['Context']\n",
    "        return question, context\n",
    "\n",
    "# Call the function and print the results\n",
    "question_with_asthma, context_3 = get_question_and_context_for_asthma(df)\n",
    "\n",
    "# Example question based on the context\n",
    "question = \"When are you more likely to get a blood clot?\"\n",
    "\n",
    "# Constructing the prompt\n",
    "prompt = f\"Context: {context_3}\\n\\nQuestion: {question}\\n\\nInstruction: Based on the context, answer the question.\\n\"\n",
    "\n",
    "\n",
    "# Tokenize and generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full model's response for debugging\n",
    "print(\"\\nFull Model's Response:\\n\", response)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer = response.split('\\n')[-1].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Model's Response:\n",
      " Context: There's a lot riding on your spinal column. It's your body's main structural support. It needs to keep you stable enough to stand upright but flexible enough for movement. So it's no surprise that many people have back problems from time to time. The hurt can stem from sore muscles, ligaments, and tendons, or from herniated disks, fractures, and other problems in your upper, middle, and lower back. Sometimes you feel the effects right away. But in many cases, back problems develop over time. We often bring on our back problems through bad habits, such as: Poor posture, like sitting incorrectly at a desk or behind the steering wheel Repeating the same motion or overdoing it Pushing, pulling, and lifting things carelessly The spine is actually a stack of 24 bones called vertebrae. A healthy spine is S-shaped when viewed from the side. It curves back at your shoulders and inward at your neck and small of your back. It houses and protects your spinal cord, the network of nerves that transmit feeling and control movement throughout your entire body. One of the more common types of back pain comes from straining the bands of muscles surrounding the spine. It happens most often in the curve of the low back and the base of the neck. These areas support more weight than your upper and mid back, which are less prone to trouble. Injuries from contact sports, accidents, and falls can cause problems ranging from minor muscle strains, to herniated disks, to fractures that damage to the spinal column or cord. Stabbing low back pain could be from muscle spasms, when your muscles seize up and don't relax, like a cramp. Osteoarthritis can affect your vertebrae, when the cartilage between them wears down. Bone spurs or a herniated disk can push on nerves. Pregnancy often brings on back pain, too. Hormonal changes and weight gain put new kinds of stresses on a pregnant woman's spine and legs. Sometimes your back might be sore for no clear reason. That's called nonspecific backache. It could come from weak muscles that can't handle everyday walking, bending, and stretching. Back pain -- whether a dull ache or shooting -- is just one sign that something's going on with your back. You may also have feelings in your legs or arms: Radiating pain Numbness Tingling Weakness Uncontrolled peeing or pooping could mean a serious problem like spinal cord compression. Call your doctor right away. You should see a doctor: After you get hurt, like in a fall or accident When the pain gets in the way of your daily activities If it lasts longer than 6 weeks, or spreads During your exam, your doctor will test your range of motion -- unless you can't move -- and check how well your nerves are working. That may be enough to decide what to do next. You might need imaging tests, like X-rays, an MRI, or a CT scan. But they're not always useful, and there's not always a direct link between the results of these tests and how much it hurts. Your specific treatment will depend on what's causing your pain and where in your back it is. Despite what you may think or have been told before, staying in bed isn't usually the answer; gentle exercise is. It will help work out the kinks, build support for your spine, and improve your flexibility. A physical therapist can work with you to design a set of exercises, give you relief from the pain, and get you moving again. Over-the-counter pain relievers, ice, and heat will work to take the edge off most back pain. Your doctor can prescribe stronger medicines, but some can make you drowsy or dependent on them if you're not careful. Complementary therapies, such as chiropractic spinal manipulation, acupuncture, and massage, can help ease pain, too. If a bone is damaged, or you have a herniated disk or pinched nerve, you may need surgery. But for ongoing back pain, doctors will try other treatments first. Counseling could help you learn to live with chronic pain better as well as deal with symptoms of depression because of it. Exercise! Strengthening the muscles around your spine and in your core will help keep you stable and balanced. Walking is great for your low back, and it's simple to do. Practice good posture. As a rule of thumb, aim to keep your ears, shoulders, and hips aligned when you sit, stand, and walk. Lift heavy things correctly, using your hips and knees for power while keeping your back straight. Try sleeping on your side, with a medium-firm mattress. Don't smoke. It restricts blood flow, so your muscles and tissues don't get a good supply of nutrients and oxygen. That can lead to weakness and aches. Repeated coughing could strain your back.\n",
      "\n",
      "Question: How should you lift objects to prevent back pain?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. The spine is actually a stack of 24 bones called vertebrae.\n",
      "2. A healthy spine is S-shaped when viewed from the side.\n",
      "3. It curves back at your shoulders and inward at your neck and small of your back.\n",
      "4. It houses and protects your spinal cord, the network of nerves that transmit feeling and control movement throughout your entire body.\n",
      "5. One of the more common types of back pain comes from straining the bands of muscles surrounding the spine.\n",
      "6. It happens most often in the curve of the low back and the base of the neck.\n",
      "7. These areas support more weight than your upper and mid back, which are less prone to trouble.\n",
      "8. Injuries from contact sports, accidents, and falls can cause problems ranging from minor muscle strains, to herniated disks, to fractures that damage to the spinal column or cord.\n",
      "9. Stabbing low back pain could be from muscle spasms, when your muscles seize up and don't relax, like a cramp.\n",
      "10. Osteoarthritis can affect your ver\n",
      "\n",
      "Model's Answer:\n",
      " 10. Osteoarthritis can affect your ver\n"
     ]
    }
   ],
   "source": [
    "# Function to find the question and context for questions containing 'asthma'\n",
    "def get_question_and_context_for_asthma(df):\n",
    "    # Filter the DataFrame for rows where the 'Question' column contains the word 'asthma'\n",
    "    asthma_rows = df[df['Question'].str.contains(r'\\bprevent back pain\\b', case=False, regex=True)]\n",
    "    \n",
    "    # If any rows are found, return the 'Question' and 'Context' of the first such row\n",
    "    if not asthma_rows.empty:\n",
    "        question = asthma_rows.iloc[0]['Question']\n",
    "        context = asthma_rows.iloc[0]['Context']\n",
    "        return question, context\n",
    "    else:\n",
    "        return \"No question with the keyword 'asthma' found.\", None\n",
    "\n",
    "# Call the function and print the results\n",
    "question_with_asthma, context_4 = get_question_and_context_for_asthma(df)\n",
    "\n",
    "# Example question based on the context\n",
    "question = \"How should you lift objects to prevent back pain?\"\n",
    "\n",
    "# Constructing the prompt\n",
    "prompt = f\"Context: {context_4}\\n\\nQuestion: {question}\\n\\nInstruction: Based on the context, answer the question.\\n\"\n",
    "\n",
    "\n",
    "# Tokenize and generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full model's response for debugging\n",
    "print(\"\\nFull Model's Response:\\n\", response)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer = response.split('\\n')[-1].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Model's Response:\n",
      " Context: Looking for an effective flu treatment and wondering if antibiotics will work? Antibiotics are medications that fight infections caused by bacteria, but the flu is cause by a virus. Taking antibiotics when you have a virus may do more harm than good. Taking antibiotics when they are not needed increases your risk of getting an infection later that may resist antibiotic treatment. Antibiotics only cure certain infections due to bacteria -- and if taken carelessly, you may get more serious health problems than you bargained for. With any illness, it is critical to address the underlying cause, whether it's bacterial or viral. Antibiotics will not kill cold or flu viruses. Not at all. Antibiotics can save people's lives, and if you need them, you should get them as quickly as you can. Since only a doctor can prescribe antibiotics, this means that you should talk to your doctor if you think you might need them (as opposed to taking your friend's leftover antibiotics from last winter's illness, for example). However, it is the grave over-reliance and inappropriate use of antibiotics that have contributed to the global antibiotic resistance crisis that we face. A study by the CDC showed that many adults believe that if they are sick enough to see a doctor for a cold, they should get an antibiotic treatment. The study also showed that patients are not aware of the consequences of taking the drugs if they are not needed. And when antibiotics are misused, bacteria can become resistant. Antivirals are medications that reduce the ability of flu viruses to multiply. The CDC considers antiviral drugs as a \"second line of defense against the flu.\" The first line of defense is getting an annual flu vaccine. When taken at the onset of flu, these drugs help decrease the severity and duration of flu symptoms. They can also be used in cases to help prevent the flu, but they are not a replacement for getting the flu vaccine. The CDC recommends baloxavir marboxil ( Xofluza), oseltamivir ( Tamiflu), peramivir ( Rapivab), and zanamivir ( Relenza) for flu. They are most effective when given within 48 hours after symptoms start to appear. These flu drugs can decrease the duration of the flu by one to two days if used within this early time period. Oseltamivir ( Tamiflu), and zanamivir ( Relenza) are usually given for a period of five days to treat the flu. For flu prevention, they are typically used for at least 7 days. In some cases, antivirals may be given for longer periods of time. For prevention of flu, antiviral drugs may be given for at least 7 days. In some cases, antivirals may be given for longer periods of time. Oseltamivir is approved for treatment in those over 2 weeks of age and for prevention in people ages 3 months and older. Peramivir, given in one intravenous dose, is approved for people ages 2 and older. Zanamivir, an inhaled medication, is approved for treatment of people ages 7 and older and for prevention in people ages 5 and older. Side effects of antiviral drugs may include nervousness, poor concentration, nausea, vomiting, and diarrhea. Zanamivir is not recommended for people with a history of breathing problems, such as asthma, because it may worsen breathing. Discuss side effects with your doctor. According to the CDC, antibiotic resistance happens when bacteria changes in some way to reduce or eliminate the effectiveness of the antibiotic. When bacteria are exposed to antibiotics repeatedly, such as when you take the medication needlessly or too frequently, the germs in your body start to evolve. These changes can make the germs stronger than before so they completely resist the antibiotic. Yourillness may linger with no signs of improvement. Or your illness may suddenly take a turn for the worse, requiring you to seek emergency medical care. You may have to be admitted to the hospital and get several different antibiotics intravenously. Sadly, those around you may get the resistant bacteria and come down with a similar illness that is very difficult to treat. Unfortunately, demand for a \"quick fix\" for what ails us has fueled this resistance crisis. The CDC estimates that about one in three antibiotic prescriptions written in the United States is unnecessary. There is a way to protect yourself and others from resistant bacteria, and that is to respect antibiotics and take them only when necessary for a bacterial infection. Here are some useful tips: When you see a doctor, don't demand antibiotics. Understand that antibiotics are used for bacterial infections, not symptoms of a cold or flu virus. If a doctor prescribes antibiotics, use them as prescribed. Take all of the antibiotics as directed and don't save some for future use. Don't share antibiotics with others. Preventing the flu in the first place may help you avoid getting sick altogether. Get a flu shot each year. Also, make sure you wash your hands frequently and thoroughly to prevent spreading germs.\n",
      "\n",
      "Question: How can you be smart with antibiotics?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. Antibiotics are medications that fight infections caused by bacteria, but the flu is cause by a virus. Taking antibiotics when you have a virus may do more harm than good.\n",
      "2. Antibiotics only cure certain infections due to bacteria -- and if taken carelessly, you may get more serious health problems than you bargained for.\n",
      "3. With any illness, it is critical to address the underlying cause, whether it's bacterial or viral.\n",
      "4. Antibiotics will not kill cold or flu viruses. Not at all.\n",
      "5. Antibiotics can save people's lives, and if you need them, you should get them as quickly as you can.\n",
      "6. However, it is the grave over-reliance and inappropriate use of antibiotics that have contributed to the global antibiotic resistance crisis that we face.\n",
      "7. A study by the CDC showed that many adults believe that if they are sick enough to see a doctor for a cold, they should get an antibiotic treatment.\n",
      "8. The study also showed that patients are not aware of the consequences of taking\n",
      "\n",
      "Model's Answer:\n",
      " 8. The study also showed that patients are not aware of the consequences of taking\n"
     ]
    }
   ],
   "source": [
    "def get_context_for_specific_question(df, question):\n",
    "    # Filter the DataFrame for the specific question\n",
    "    filtered_df = df[df['Question'] == question]\n",
    "\n",
    "    # Check if there are any matching rows\n",
    "    if not filtered_df.empty:\n",
    "        # Return the context of the first matching row\n",
    "        return filtered_df.iloc[0]['Context']\n",
    "    else:\n",
    "        return \"No context found for the specified question.\"\n",
    "\n",
    "# Call the function with the specific question\n",
    "specific_question = \"Do most people know that antibiotics are not effective for colds or flu?\"\n",
    "context_5 = get_context_for_specific_question(df, specific_question)\n",
    "\n",
    "# Example question based on the context\n",
    "question = \"How can you be smart with antibiotics?\"\n",
    "\n",
    "# Constructing the prompt\n",
    "prompt = f\"Context: {context_5}\\n\\nQuestion: {question}\\n\\nInstruction: Based on the context, answer the question.\\n\"\n",
    "\n",
    "\n",
    "# Tokenize and generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full model's response for debugging\n",
    "print(\"\\nFull Model's Response:\\n\", response)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer = response.split('\\n')[-1].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Model's Response:\n",
      " Context: There are different types of inhalers that serve different purposes and require different techniques. These inhalers help prevent flares and keep symptoms from getting worse. They're called control inhalers because they have medicine that controls inflammation. Use yours as often as your doctor tells you to, usually once or twice a day: Whether or not you're having symptoms Even if you feel like you're doing better If you're supposed to use it two times a day, aim for 12 hours apart. When you begin using this kind of inhaler, it may be 2 to 4 weeks before you notice the drugs start to work. Rescue or relief inhalers quickly bring back normal breathing when you are: Short of breath Wheezing Feeling tight in your chest Coughing You should keep a rescue inhaler with you all the time. Use it: When you have a flare of symptoms Before you're going to be around your asthma triggers When you run into unexpected triggers A rescue inhaler is for short-term symptom relief, not to control your asthma in the long term. If you're using yours 2 or more days a week, or more than 2 nights a month, talk to your doctor about a daily control inhaler. When you have asthma triggered by exercise, short-acting inhalers can make activities that need extra lung power more doable. This includes things such as sports, yard work, and even singing. To help prevent symptoms, use your rescue inhaler 15 to 30 minutes before you start. Keep it on hand in case you have symptoms while you're working. If lively movement often brings on a flare, don't give up on exercise. Regular exercise can help you control your asthma. It can strengthen lung muscles, make it easier to manage your weight, and boost your immune system. Instead: Try different kinds of activities that are less challenging. Avoid weather conditions that might trigger symptoms. Inhalers are different, so check your instructions. Prime the inhaler first. You need to do this when you use an inhaler for the first time, or if you haven't used it for 2 weeks or more. Shake it for 5 seconds, turn the inhaler away from you, and press down to spray it. Wait a few seconds and do it again. Then do this two more times for a total of four. Take off the mouthpiece cover, then: Shake it for 5 seconds. Hold the inhaler up with your index finger on top and your thumb underneath to support it. Use the other hand to hold the spacer if you need to. Breathe out. Put the mouthpiece between your teeth, and close your lips tightly around it. ( Make sure your tongue doesn't block the opening.) You can also hold the mouthpiece about the width of two fingers away from your mouth. Press the top down, and breathe in until your lungs fill completely -- about 4-6 seconds. Hold the medicine in your lungs as long as you can (5-10 seconds is good), then breathe out. If you don't get enough air in the first breath, wait 15-30 seconds and try again. Shake the canister again before the next puff. Recap the mouthpiece. If your medicine has a steroid in it, rinse your mouth and gargle with water after you use the inhaler. Spit out the water. Put the inhaler into the spacer. Shake it for 5 seconds. Hold the inhaler up with your index finger on top and your thumb underneath to support it. Use the other hand to hold the spacer if you need to. Breathe out. Put the mouthpiece between your teeth, and close your lips tightly around the spacer. ( Make sure your tongue doesn't block the opening.) Press the top down and breathe in until your lungs fill completely -- about 3-5 seconds. Hold the medicine in your lungs as long as you can (5-10 seconds is good), then breathe out. If you don't get enough air in the first breath, wait 15-30 seconds and try again. Shake the inhaler again before the second puff. Don't fill the chamber with two puffs of medicine at once. Recap the mouthpiece. If your medicine has a steroid in it, rinse your mouth and gargle with water after you use the inhaler. Spit out the water. Remove the cap. For a single-use device, load a capsule. Breathe out slowly (not into the mouthpiece). Put the mouthpiece between your front teeth and close your lips around it. Breathe in through your mouth deeply for 2-3 seconds. Remove the inhaler. Hold your breath for as long as you can. ( Between 4 and 10 seconds is good.) Breathe out slowly. You have to clean them about once a week so the medication doesn't build up and block the mouthpiece. MDI: Remove the canister and cap from the mouthpiece. Don't wash the canister or put it in water. Run warm tap water through the top and bottom of the mouthpiece for 30-60 seconds. Use a soft cloth to remove any medication buildup. Shake off the water. Let the mouthpiece dry completely. Overnight is best. If you need to use the inhaler before the mouthpiece dries, shake off the extra water, replace the canister, point it away from your face, and test-spray it twice before you use it. DPI: Don't wash it with soap and water. Clean the mouthpiece with a dry cloth. Check the instructions for more information.\n",
      "\n",
      "Question: What types of exercise are best for people with asthma?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. Swimming\n",
      "2. Walking\n",
      "3. Cycling\n",
      "4. Running\n",
      "\n",
      "Question: What is the best way to use an inhaler?\n",
      "\n",
      "Instruction: Based on the context, answer the question.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. Shake the inhaler for 5 seconds.\n",
      "2. Hold the inhaler up with your index finger on top and your thumb underneath to support it.\n",
      "3. Breathe out.\n",
      "4. Put the mouthpiece between your teeth, and close your lips tightly around it.\n",
      "5. Press the top down, and breathe in until your lungs fill completely -- about 4-6 seconds.\n",
      "6. Hold the medicine in your lungs as long as you can (5-10 seconds is good), then breathe out.\n",
      "7. If you don't get enough air in the first breath, wait 15-30 seconds and try again.\n",
      "8. Shake the canister again before the next puff.\n",
      "9. Recap the mouthpiece.\n",
      "10. If your medicine has a steroid in it, rinse your mouth and gargle\n",
      "\n",
      "Model's Answer:\n",
      " 10. If your medicine has a steroid in it, rinse your mouth and gargle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame already loaded with data\n",
    "\n",
    "def get_context_for_specific_question(df, question):\n",
    "    # Filter the DataFrame for the specific question\n",
    "    filtered_df = df[df['Question'] == question]\n",
    "\n",
    "    # Check if there are any matching rows\n",
    "    if not filtered_df.empty:\n",
    "        # Return the context of the first matching row\n",
    "        return filtered_df.iloc[0]['Context']\n",
    "    else:\n",
    "        return \"No context found for the specified question.\"\n",
    "\n",
    "# Call the function with the specific question\n",
    "specific_question = \"Is exercise OK if you have asthma?\"\n",
    "context_1 = get_context_for_specific_question(df, specific_question)\n",
    "\n",
    "\n",
    "# Example question based on the context\n",
    "question = \"What types of exercise are best for people with asthma?\"\n",
    "\n",
    "# Constructing the prompt\n",
    "prompt = f\"Context: {context_1}\\n\\nQuestion: {question}\\n\\nInstruction: Based on the context, answer the question.\\n\"\n",
    "\n",
    "\n",
    "# Tokenize and generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "max_length_adjusted = min(len(inputs['input_ids'][0]) + 250, tokenizer.model_max_length)\n",
    "outputs = base_model.generate(**inputs, max_length=max_length_adjusted, top_p=0.9, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full model's response for debugging\n",
    "print(\"\\nFull Model's Response:\\n\", response)\n",
    "\n",
    "# Extracting only the answer part\n",
    "answer = response.split('\\n')[-1].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(mashqa_df, page_content_column=\"Context\")\n",
    "# loader = DataFrameLoader(mashqa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "# # Use lazy load for larger table, which won't read the full table into memory\n",
    "# docs = loader.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chunked_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect query to FAISS index using a retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One component of red yeast rice, monacolin K, is similar to the active ingredient of some of the cholesterol lowering drugs called statins. Red yeast rice does not seem to be as effective as most conventional statin drugs, however. But these supplements may be a good choice for people who could benefit from only a slight lowering of cholesterol.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Laporta say?\"\n",
    "docs = db.similarity_search(query)\n",
    "# print(docs[0].page_content)\n",
    "print(docs[0].metadata['Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_generation_pipeline = transformers.pipeline(\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    #return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Write a response to answer the questionby taking help from below context:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "There are several types of exercises that can be beneficial for people with asthma. Low-impact exercises such as swimming, cycling, and walking are generally recommended as they are less likely to trigger asthma symptoms. Yoga and tai chi are also good options as they promote relaxation and can help improve breathing techniques. Strength training exercises can also be helpful in improving overall fitness and reducing the risk of asthma exacerbations. However, it is important for individuals with asthma to consult with their healthcare provider before starting any new exercise program to ensure that it is safe and appropriate for their individual needs and limitations.\n"
     ]
    }
   ],
   "source": [
    "query = \"What types of exercise are best for people with asthma?\" \n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "# Invoke the model and capture the response\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Extracting the answer\n",
    "answer = response['text'].strip()\n",
    "\n",
    "# Print only the model's answer\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "Obsessive-compulsive disorder (OCD) is typically diagnosed through a combination of self-reporting, clinical interview, and psychological testing. The diagnostic criteria for OCD are established by the Diagnostic and Statistical Manual of Mental Disorders (DSM), which is a widely accepted classification system used by mental health professionals to diagnose and treat mental health disorders.\n",
      "\n",
      "To diagnose OCD, a mental health professional will typically conduct a comprehensive evaluation that includes gathering information about the individual's symptoms, medical history, and family history. The evaluation may also involve administering psychological tests, such as the Yale-Brown Obsessive Compulsive Scale (YBOCS), to assess the severity and frequency of the individual's symptoms.\n",
      "\n",
      "In addition to the above, the individual may also be referred for a neuropsychological assessment to evaluate their cognitive functioning and rule out any underlying neurological conditions that may be contributing to their symptoms.\n",
      "\n",
      "It is important to note that a diagnosis of OCD requires a thorough evaluation by a qualified mental health professional, and a proper diagnosis can only be made after a comprehensive evaluation.\n"
     ]
    }
   ],
   "source": [
    "query = \"How is obsessive-compulsive disorder diagnosed?\"\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "answer = response['text'].strip()\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "A blood clot occurs when blood clots together, forming a blockage in a blood vessel. Blood clots can occur in any part of the body, but they are most commonly found in the legs, arms, and lungs. There are several factors that can increase your risk of getting a blood clot. Here are some of the most common ones:\n",
      "\n",
      "1. Age: As you age, your blood vessels become stiffer and less flexible, making it easier for blood clots to form.\n",
      "2. Smoking: Smoking damages the lining of your blood vessels, making it easier for blood clots to form.\n",
      "3. Obesity: Being overweight or obese can increase your risk of getting a blood clot, especially if you have high blood pressure or high cholesterol.\n",
      "4. Sedentary lifestyle: Leading a sedentary lifestyle, such as sitting for long periods of time, can increase your risk of getting a blood clot.\n",
      "5. Family history: If you have a family history of blood clots, you may be at higher risk of getting one yourself.\n",
      "6. Medical conditions: Certain medical conditions, such as heart disease, diabetes, and cancer, can increase your risk of getting a blood clot.\n",
      "7. Medications: Taking certain medications, such as birth control pills, hormone replacement therapy, and anticoagulants, can increase your risk of getting a blood cl\n"
     ]
    }
   ],
   "source": [
    "query = \"When are you more likely to get a blood clot?\"\n",
    "response = rag_chain.invoke(query)\n",
    "answer = response['text'].strip()\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How should you lift objects to prevent back pain?\"\n",
    "response = rag_chain.invoke(query)\n",
    "answer = response['text'].strip()\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pp/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's Answer:\n",
      "\n",
      "To be smart with antibiotics, it is important to follow the instructions provided by your healthcare provider and complete the full course of treatment as prescribed. Antibiotics are designed to kill specific types of bacteria, and taking them for too short a period of time or skipping doses can allow the remaining bacteria to multiply and become resistant to the antibiotic. Additionally, it is important to avoid taking antibiotics unnecessarily, as overuse can contribute to the development of antibiotic-resistant bacteria. Finally, it is important to practice good hygiene and take steps to prevent the spread of infection, such as washing your hands regularly and covering your mouth and nose when coughing or sneezing.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can you be smart with antibiotics?\"\n",
    "response = rag_chain.invoke(query)\n",
    "answer = response['text'].strip()\n",
    "print(\"\\nModel's Answer:\\n\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMLzDqBFb47IrSzjv+iC3Ea",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "mar39",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03763433ff9a4fd2874abb9ad034260a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a93054e51a3460fb26e46d50df5de36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16ca639215604d50922eb43cf514c30b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d69f7aad6c457f8ca79a87152e8b70",
      "max": 1795303,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17a7bb570b914517aa474209136afa67",
      "value": 1795303
     }
    },
    "16fd3fa967434963b784627e1ad743ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "17a7bb570b914517aa474209136afa67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "18b91caca9c9438bb97fbd84f81e07b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19dcb76000734120ba1838753e1e29e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c9f309ef4474982b496aed308e64541",
       "IPY_MODEL_8d9d4d292d69409689f903d99a59fda0",
       "IPY_MODEL_4abac84ee3e84415ac5d502964f97e72"
      ],
      "layout": "IPY_MODEL_68876547146d47a5b0bbce69e2a763d6"
     }
    },
    "1f9b9ac166294206ace77b3a50054a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2101ed0500c342b799177e1ccaacf1f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27af2ea65da64811a3b85974136cff4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c9f309ef4474982b496aed308e64541": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03763433ff9a4fd2874abb9ad034260a",
      "placeholder": "​",
      "style": "IPY_MODEL_9409919e903a4ebe85c6338ba1bff4aa",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "34e3250a8f1840e8abc6f31a0def4ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a07408681de4644bd35a366866bf69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43252bf33e44478db788d9897c629b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4abac84ee3e84415ac5d502964f97e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b03fe16c4e624e88a85e2a6a68bbb8b5",
      "placeholder": "​",
      "style": "IPY_MODEL_1f9b9ac166294206ace77b3a50054a3d",
      "value": " 966/966 [00:00&lt;00:00, 74.8kB/s]"
     }
    },
    "4b85cd5c84694ac88d2d8ec8e97849f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9196e29878524e1f856fea3508b99bf9",
      "placeholder": "​",
      "style": "IPY_MODEL_996185e3aa09432bacab12407c53596d",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "4c0cfb4be4f743a9af8c8468374eec92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d8f32abfda54301809247936b9b54d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a7187fc9452486aad2d907dd414ffd5",
       "IPY_MODEL_bcb97b7c3e374b6d97c71d9b7f7e4062",
       "IPY_MODEL_9d89be2492294db19d267fc6acd18604"
      ],
      "layout": "IPY_MODEL_2101ed0500c342b799177e1ccaacf1f8"
     }
    },
    "4dcdd3115d9d4a1bb305466a57b10cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a93db9179f4491f92126e813e0d462f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b6ba938780549879712eae15a236caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a726eb7fc547ee9a465398c9f45e2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6239e1e2a65d4733ae780d6ecaa3a4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c89e9320a01d460eb3135c03e9f5dc19",
      "placeholder": "​",
      "style": "IPY_MODEL_43252bf33e44478db788d9897c629b05",
      "value": "tokenizer.json: 100%"
     }
    },
    "68876547146d47a5b0bbce69e2a763d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74217aaeb5114a158cdf0ca8c81b919f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe849d1fa1124dd88526a278765a7e6a",
      "placeholder": "​",
      "style": "IPY_MODEL_3a07408681de4644bd35a366866bf69f",
      "value": " 1.80M/1.80M [00:00&lt;00:00, 45.3MB/s]"
     }
    },
    "799feff152df49e9a1ebc26ed42f2995": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18b91caca9c9438bb97fbd84f81e07b2",
      "placeholder": "​",
      "style": "IPY_MODEL_34e3250a8f1840e8abc6f31a0def4ee1",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "7a7187fc9452486aad2d907dd414ffd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61a726eb7fc547ee9a465398c9f45e2e",
      "placeholder": "​",
      "style": "IPY_MODEL_a3634a74e648497584ea686fcf28a2c2",
      "value": "tokenizer.model: 100%"
     }
    },
    "8959c048bfb344818fbd3099b02dd31f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7b2e06c1561455998546a5ddd05997a",
      "placeholder": "​",
      "style": "IPY_MODEL_ea92b0d6ea20460c946169045c2a4faa",
      "value": " 2/2 [00:16&lt;00:00,  7.81s/it]"
     }
    },
    "8d9d4d292d69409689f903d99a59fda0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c0cfb4be4f743a9af8c8468374eec92",
      "max": 966,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc69790114db41bcafcabd8cff380b50",
      "value": 966
     }
    },
    "9196e29878524e1f856fea3508b99bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91fc5b8b2ab44172acb853d283a2e6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_799feff152df49e9a1ebc26ed42f2995",
       "IPY_MODEL_9fbd6643b4a442bfa44377e19d34d453",
       "IPY_MODEL_8959c048bfb344818fbd3099b02dd31f"
      ],
      "layout": "IPY_MODEL_4dcdd3115d9d4a1bb305466a57b10cc8"
     }
    },
    "93c89ab1bb244a2aa8cf51e4c1a1039b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9409919e903a4ebe85c6338ba1bff4aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "996185e3aa09432bacab12407c53596d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d89be2492294db19d267fc6acd18604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93c89ab1bb244a2aa8cf51e4c1a1039b",
      "placeholder": "​",
      "style": "IPY_MODEL_5a93db9179f4491f92126e813e0d462f",
      "value": " 493k/493k [00:00&lt;00:00, 34.2MB/s]"
     }
    },
    "9fbd6643b4a442bfa44377e19d34d453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5038960f8dd4de2bb54fc6613d6bdd1",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6e011cb6e4e47cebb17b611437ef8b9",
      "value": 2
     }
    },
    "a2d69f7aad6c457f8ca79a87152e8b70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3634a74e648497584ea686fcf28a2c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a36af20c961e4c59a0c7af92ce331c4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e011cb6e4e47cebb17b611437ef8b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa6cb6d1f234466189afbd7f87f9fa89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b85cd5c84694ac88d2d8ec8e97849f4",
       "IPY_MODEL_ce2599a839074d1393df2eb43a8be949",
       "IPY_MODEL_da0353eab48340a6a42b5336e0f199ed"
      ],
      "layout": "IPY_MODEL_f12b3f50f91b4f5d9b8c3eb6b356c76d"
     }
    },
    "ae54ca82c7fc425dbe3c3a3227598bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6239e1e2a65d4733ae780d6ecaa3a4ee",
       "IPY_MODEL_16ca639215604d50922eb43cf514c30b",
       "IPY_MODEL_74217aaeb5114a158cdf0ca8c81b919f"
      ],
      "layout": "IPY_MODEL_a36af20c961e4c59a0c7af92ce331c4e"
     }
    },
    "b03fe16c4e624e88a85e2a6a68bbb8b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5038960f8dd4de2bb54fc6613d6bdd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcb97b7c3e374b6d97c71d9b7f7e4062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27af2ea65da64811a3b85974136cff4e",
      "max": 493443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4a6b813b3914a2fb13650597bb3c21f",
      "value": 493443
     }
    },
    "c4a6b813b3914a2fb13650597bb3c21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7b2e06c1561455998546a5ddd05997a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c89e9320a01d460eb3135c03e9f5dc19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc69790114db41bcafcabd8cff380b50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce2599a839074d1393df2eb43a8be949": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d62f9cf79a5e43fbbf854620bdbba355",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16fd3fa967434963b784627e1ad743ae",
      "value": 72
     }
    },
    "d62f9cf79a5e43fbbf854620bdbba355": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0353eab48340a6a42b5336e0f199ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a93054e51a3460fb26e46d50df5de36",
      "placeholder": "​",
      "style": "IPY_MODEL_5b6ba938780549879712eae15a236caf",
      "value": " 72.0/72.0 [00:00&lt;00:00, 5.63kB/s]"
     }
    },
    "ea92b0d6ea20460c946169045c2a4faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f12b3f50f91b4f5d9b8c3eb6b356c76d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe849d1fa1124dd88526a278765a7e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
